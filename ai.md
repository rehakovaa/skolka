#### bayesovkÃ¡ sÃ­Å¥
-  je to dag, kde kaÅ¾dÃ¡ promÄ›nnÃ¡ je vrchol a je napojenÃ¡ na svÃ©ho rodiÄe, pokud je na nÄ›m zÃ¡vislÃ¡ (hrana: chÅ™ipka -> horeÄka)
    - kaÅ¾dÃ½ uzel mÃ¡ podmÃ­nÄ›nou pravdÄ›podobnost ke svÃ½m rodiÄÅ¯m
- vyjadÅ™uje full joint probability distribution tak, aby to bylo vÃ­c kompaktnÃ­ a pÅ™ehlednÄ›jÅ¡Ã­ -> ne velkÃ¡ tabulka, ale dag

- vyuÅ¾Ã­vÃ¡me zde Å™etÄ›zovÃ© pravidlo (viz. seÅ¡it) 
    - pÅ™Ã­klad: P(dÃ©Å¡Å¥, kluzkÃ¡ silnice, nehoda) = P(dÃ©Å¡Å¥).P(kluzkÃ¡ silnice|dÃ©Å¡Å¥).P(nehoda|kluzkÃ¡ silnice)

- konstrukce:
    - definujeme mnoÅ¾inu nÃ¡hodnÃ½ch promÄ›nnÃ½ch a uspoÅ™Ã¡dÃ¡me je jako $X_1$, ..., $X_n$, aby pÅ™Ã­Äiny byly pÅ™ed dÅ¯sledky
    - $X_1$ nebude mÃ­t pÅ™edka a $X_i$ vyrobÃ­me tak, Å¾e z mnoÅ¾iny {$X_1$,...,$X_{i-1}$} vybereme nejmenÅ¡Ã­ podmnoÅ¾inu takovou, aby platilo $P(X_i|Parents(X_i)) = (P(X_i|mnoÅ¾ina))$
    - proÄ to funguje? 

- chain rule je pravidlo, kterÃ© vyjadÅ™uje spoleÄnou pravdÄ›podobnost vÃ­ce nÃ¡hodnÃ½ch promÄ›nnÃ½ch pomocÃ¡ podmÃ­nÄ›nÃ© pravdÄ›podobnosti 
$P(X_1,...,X_n) = P(X_1)*P(X_2|X_1)*...*P(X_n|X_1,...,X_{n-1})$
    - celkovÃ© spoleÄnÃ© pravdÄ›podobnostnÃ­ rozdÄ›lenÃ­ mÅ¯Å¾eme zÃ­skat, pokud znÃ¡me pravdÄ›podobnosti jednotlivÃ½ch uzlÅ¯ v sÃ­ti a jejich rodiÄe
    - to je ta vÃ½hoda: nemsuÃ­me znÃ¡t pravdÄ›podobnost vÅ¡ech kombinacÃ­, staÄÃ­ znÃ¡t podmÃ­nÄ›nÃ© zÃ¡vislosti

pÅ™Ã­klad: 
- C: zda prÅ¡Ã­
- S: je mokrÃ¡ zem
- T: vezmu si deÅ¡tnÃ­k

$Câ†’Sâ†’T$ pak $P(C,S,T) = P(C)*P(S|C)*P(T|S)$

- interference mi zjistÃ­ promÄ›nnÃ© na zÃ¡kladÄ› znÃ¡mÃ½ch hodnot ostatnÃ­ch promÄ›nnÃ½ch
    - enumerace: provede vÃ½poÄet podle ÃºplnÃ© kombinace vÅ¡ech moÅ¾nÃ½ch hodnot 
        - je to ale dost nÃ¡roÄnÃ©, vyuÅ¾Ã­t dynamickÃ© programovÃ¡nÃ­, abych zabrÃ¡nila opakovanÃ©mu poÄÃ­tÃ¡nÃ­ stejnÃ©ho ÄÃ­sla
    - eliminace promÄ›nnÃ½ch: eliminuje skrytÃ© promÄ›nnÃ© pomocÃ­ marginalizace (vysÄÃ­tÃ¡me P ze vÅ¡ech z)
    ![alt text](image.png)
    - odmÃ­tacÃ­ vzorkovÃ¡nÃ­ (reject sampling): generuje nÃ¡hodnÃ© vzorky z celÃ© sÃ­tÄ› a vyhodÃ­ ty vzorky, kterÃ© vypadajÃ­ fujky
        - sample by mÄ›l bÃ½t generovÃ¡n z known probability distribution
        - vybereme pouze ty, co sedÃ­ evidenci, ale tÄ›ch mÅ¯Å¾e bÃ½t mÃ¡lo
        - funguje i bez pÅ™esnÃ©ho modelu
    - vÃ¡Å¾enÃ© vzorkovÃ¡nÃ­: prostÄ› vylepÅ¡enÃ© vyhazovÃ¡nÃ­, evidence se nezahazuje, ale vÃ¡Å¾Ã­ se podle toho, jak odpovÃ­dÃ¡ evidenci
        - fixnu hodnoty tÄ›ch evidencÃ­ a kaÅ¾dÃ½ vzorek dostane vÃ¡hu podle toho, jak pravdÄ›podobnÃ¡ je evidence pÅ™i danÃ© konfiguraci
    - vzorek odpovÃ­dÃ¡ evidenci (vzorek je konzistentÃ­ s e) = vzorek mÃ¡ stejnÃ© hodnoty, jako to, co je zadanÃ©
        - vÃ­me, Å¾e $alarm = True$, tak vyhodÃ­me ty vzorky, co majÃ­ $alarm=False$
    - vyuÅ¾Ã­vÃ¡ se na to monte carlo technika


### probabilistic reasoning over time
- tohle se tÃ½kÃ¡ modelovÃ¡nÃ­ systÃ©mÅ¯, kterÃ© se vyvÃ­jejÃ­ v Äase, pÅ™iÄemÅ¾ nemusÃ­me znÃ¡t jejich vnitÅ™nÃ­ stav tak dobÅ™e, ale mÃ¡me pozorovÃ¡nÃ­

#### define transition and observation models and explain their assumptions

- pÅ™echodovÃ½ model popisuje, jak se stav mÄ›nÃ­ v Äase $P(X_t|X_{0:t-1})$, tedy pravdÄ›podobnost stavu na zÃ¡kladÄ› pÅ™edchozÃ­ho 
    - pokud vyuÅ¾Ã­vÃ¡me Markov asumption, staÄÃ­ nÃ¡m pouze $P(X_t|X_{t-1})$, na ostatnÃ­ch to nenÃ­ zÃ¡vislÃ©
    - stav zÃ¡visÃ­ pouze na pÅ™edchozÃ­m stavu
    - pokud jsou vÅ¡echny hodnoty stejnÃ©, dostÃ¡vÃ¡me statistickÃ½ model
- observation model popisuje, jakÃ¡ pozorovÃ¡nÃ­ oÄekÃ¡vÃ¡me, pokud znÃ¡me skuteÄnÃ½ stav (kdyÅ¾ prÅ¡Ã­, bvude pravdÄ›podobnÄ›jÅ¡Ã­, Å¾e uvidÃ­m mokrÃ© silnice)
    - pozorovÃ¡nÃ­ zÃ¡visÃ­ pouze na aktuÃ¡lnÃ­m stavu

- mÅ¯Å¾eme udÄ›lat bayesovskou sÃ­Å¥
    - $P(E_t|X_{0:t}, E_{0:t-1})$, ale zas vyuÅ¾ijeme Markova a dostaneme, Å¾e nÃ¡s zajÃ­mÃ¡ jenom $(E_t|X_t)$
    - lze to modelovat pomocÃ­ bayesovskÃ© sÃ­tÄ›, kde chain rule bude $P(X_{0:t} = P(x_0)\prod P(X_t|X_{t-1}))$ a $P(X_{0:t},E_{1:t} = P(x_{0})\prod P(X_t|X_{t-1}*P(E_t|X_t)))$

    ![alt text](image-1.png)

#### basic inference tasks
- filtering - vÃ½poÄet aktuÃ¡lnÃ­ho stavu na zÃ¡kladÄ› vÅ¡ech pÅ™edchozÃ­ch stavÅ¯ aneb 'kde teÄ jsem??'
    - je potÅ™eba jÃ­t od zaÄÃ¡tku, pouÅ¾Ã­vÃ¡ se rekurzivnÄ› pomocÃ­ pÅ™edchozÃ­ho vÃ½sledku 
    - $f_{1:0} = P(X_0)$ a potÃ© se jde dÃ¡l jako $f_{1:t} = P(X_t|e_{1:t}) $ a $f_{1:t+1} = \alpha *forward(f_{1:t},e_{t+1})$
        - kde to prvnÃ­ je pravdÄ›podobnost $X_0$
        - to druhÃ© je filtraÄnÃ­ distribuce v Äase $t$
        - a to tÅ™etÃ­ je filtraÄnÃ­ distribuce v $t+1$
    - $P(X_{t+1}âˆ£e_{1:t})=âˆ‘P(X_{t+1}âˆ£x_t)P(x_tâˆ£e_{1:t})$ nÃ¡m Å™Ã­kÃ¡, jakÃ½ je pravdÄ›podobnost, Å¾e nastal nÃ¡Å¡ stav, pokud uvÃ¡Å¾Ã­me to, co vÃ­me z minulosti 
    - a pak to dÃ¡me dohromady s tÄ›mi pozorovÃ¡nÃ­mi, co dostaneme v $t+1$
    - $P(X_{t+1}âˆ£e_{1:t+1})=Î±P(e_{t+1}âˆ£X{t+1})â‹…P(X{t+1|}e_{1:t})$  nÃ¡m Å™Ã­kÃ¡, jak upravÃ­me naÅ¡ei pÅ™edikci, pokud teÄ vidÃ­me novÃ¡ pozorovÃ¡nÃ­ 

- prediction - spoÄÃ­tat pravdÄ›podobnost budoucÃ­ho stavu promÄ›nnÃ© $X_{t+k}$, aniÅ¾ bychom mÄ›li novÃ¡ pozorovÃ¡nÃ­ po Äase $t$
    - mÃ¡me pozorovÃ¡nÃ­ do Äasu t a chceme zjistit, jakÃ½ bude nÃ¡Å¡ pravdÄ›podobnÃ½ stav v Äase dÃ¡l
    - je to jako filtering bez novÃ½ch dÅ¯kazÅ¯
    $P(X_{t+k+1}âˆ£e_{1:t})=x_{t+k}*âˆ‘P(X_{t+k+1}âˆ£x_{t+k})*P(x_{t+k}âˆ£e_{1:t})$
    - vezmi vÅ¡echny stavy moÅ¾nÃ© v Äase $t+k$ a pro kaÅ¾dÃ½ z nich spoÄÃ­tej, jakÃ¡ je pravdÄ›podobnost, Å¾e pÅ™ejdu do stavu $x_{t+k+1}$ krÃ¡t pravdÄ›podobnost, Å¾e jsem v tom danÃ©m stavu $x_{t+k}$
    - kdyÅ¾ budeme produkovat dlouho tyhle vÃ½poÄty, pÅ™estane zÃ¡leÅ¾et na tom, kde jsme zaÄali
        - konvergence ke stacionÃ¡rnÃ­ distribuci

- smoothing Å™eÅ¡Ã­, kde jsem byl v minulosti
    - chceme zjistit, jak pravdÄ›podobnÄ› vypadala situace $k<t$ (napÅ™. sledujeme robota poslednÃ­ch deset sekund a zaznamenÃ¡vÃ¡me, co dÄ›lal, tak kde byl v tÅ™etÃ­ sekundÄ›)
    - abychom spoÄÃ­tali $P(X_kâˆ£e_{1:t})$, rozdÄ›lÃ­me ten vÃ½poÄet na dvÄ› ÄÃ¡sti   
        - pravdÄ›podobnost, Å¾e systÃ©m byl ve stavu $x_k$, kdyÅ¾ znÃ¡me evidenci do Äasu $k$ - je to prostÄ› filtering
        - pravdÄ›podobnost budoucÃ­ch pozorovÃ¡nÃ­, kdyÅ¾ jsme ve stavu $x_k$
        - kde $b_k=x_{k+1}*âˆ‘P(e_{k+1}âˆ£x_{k+1})â‹…P(x_{k+1}âˆ£X_k)â‹…b_{k+1}$
        - $f_{1:k}$ je klasickÃ½ forward filtering
        - $b_{k+1:t}$ je backward massage passing (zprava doleva)

    $P(X_kâˆ£e_{1:t})=Î±â‹…P(X_kâˆ£e_{1:k})â‹…P(e_{k+1:t}âˆ£X_k) = Î±â‹…f_{1:k}*b_{k+1:t}*x_{k+1}*âˆ‘P(e_{k+1:t}âˆ£x_{k+1})â‹…P(x_{k+1}âˆ£X_k)$

- most likely explanation - nejpravdÄ›podobnÄ›jÅ¡Ã­ sekvence stavÅ¯, kterÃ¡ nejlÃ©pe vysvÄ›tluje danÃ¡ pozorovÃ¡nÃ­
    - mohlo by se to podobat smoothingu, ale tam jde jenom o jeden okamÅ¾ik, tady jde o celou sekvenci 
    - vyuÅ¾ijeme viterbiho algoritmus
        - $ğ›¿_t(x_t)$ je pravdÄ›podobnost nejlepÅ¡Ã­ cesty do stavu $x_t$ v Äase $t$
        - $P(x_{t+1}|x_t)$ je pÅ™echodovÃ¡ pravdÄ›podobnost
        - $P(e_{t+1} | x_{t+1})$ je pravdÄ›podobnost pozorovÃ¡nÃ­ 
        - je to rekurzivnÃ­ vÃ½poÄet, kdy si v kaÅ¾dÃ©m kroku uloÅ¾Ã­me nejlepÅ¡Ã­ pÅ™edchozÃ­ stav a pokraÄuju do dalÅ¡Ã­ho stavu
        - jakmile se dostanu na konec, najdu stav s nejvyÅ¡Å¡Ã­m $ğ›¿_t$ a zrekontruuju z nÄ›ho tu nejlepÅ¡Ã­ cestu
        $Î´_{t+1}(x_{t+1})=max_{x_i}[Î´_t(x_t)â‹…P(x_{t+1}âˆ£x_t)â‹…P(e_{t+1}âˆ£x_{t+1})]$

#### hidden markov model vs dynamic bayesian network
- pokud jsou nÄ›kterÃ© stavy skrytÃ© (nejsou pÅ™Ã­mo pozorovatelnÃ©) a mÃ¡me pozorovÃ¡nÃ­ $E_t$, kterÃ© zÃ¡visÃ­ na stavech, a jeÅ¡tÄ› pÅ™echod mezi stavy zÃ¡visÃ­ jenom na pÅ™edchozÃ­m stavu a pozorovÃ¡nÃ­ zÃ¡visÃ­ jen na nynÄ›jÅ¡Ã­m stavu, tak mÃ¡me HMM
    - $P(X_0)$ je poÄÃ¡teÄnÃ­ distribuce stavÅ¯ 
    - $P(X_t|X_{t-1})$ je pÅ™echodovÃ½ model (transition model) 
        - je to matice, kde $T_{(i,j)} = P(X_t=j|X_{t-1}=i)$ 
    -$P(E_t|X_t)$ je pozorovacÃ­ model 
        - taky matice
        - $O_{t(i,j)} = P(E_t=e_t|X_t=i)$
    - pÅ™i tÄ›ch maticÃ­ch vyuÅ¾Ã­vÃ¡me dopÅ™ednÃ©ho a zpÄ›tnÃ©ho Å¡Ã­Å™enÃ­ zprÃ¡v pomocÃ­ maticovÃ½ch operacÃ­ch
    - nejdÅ™Ã­v dopÅ™ednÃ© Å¡Ã­Å™enÃ­ - pravdÄ›podobnost, Å¾e se systÃ©m nachÃ¡zÃ­ ve stavu $X_t$ na zÃ¡kladÄ› naÅ¡ich pozorovÃ¡nÃ­ do Äasu $t$
        $f_{1:t+1}=Î±â‹…P(e_{t+1}âˆ£X_{t+1})â‹…âˆ‘P(X_{t+1}âˆ£x_t)â‹…P(x_tâˆ£e_{1:t})$
        - v matici pro pÅ™echody je to $T(i,j)=P(X_{t=j}âˆ£X_{tâˆ’1=i})$
        - a tu matice pro pozorovÃ¡nÃ­: $O_{t+1}(i,i)=P(E_{t+1}=e_{t+1}âˆ£X_{t+1=i})$
        - dohromady to dÃ¡vÃ¡ $f_{1:t+1}=Î±â‹…O_{t+1}â‹…T^Tâ‹…f_{1:t}$
        - vysvÄ›tlenÃ­: ten poslednÃ­ nÃ¡sobek promÃ­tne aktuÃ¡lnÃ­ odhad stavÅ¯ pÅ™es pÅ™echodovu matici; ten druhÃ½ upravÃ­ vÃ½sledek podle aktuÃ¡lnÃ­ho pozorovÃ¡nÃ­ a $\alpha$ to znormalizuje
    - teÄ zpÄ›tnÃ© Å¡Ã­Å™enÃ­, co se pouÅ¾Ã­vÃ¡ pÅ™i smoothingu
        - chceme spoÄÃ­tat $P(e_{k+1:t}âˆ£X_k)=b_{k+1:t}$ a vyuÅ¾ijeme pro to vzorec maticovÃ½ $b_{k+1:t}=Tâ‹…O_{k+1}â‹…b_{k+2:t}$
        - kde druhÃ¡ ÄÃ¡st promÃ­tnÄ› budoucÃ­ zprÃ¡vu podle pozorovÃ¡nÃ­ a $T$ promÃ­tne tuhle zprÃ¡vu zpÄ›t do pÅ™edchozÃ­ho Äasu pomocÃ­ pÅ™echodÅ¯ 
- dynamickÃ¡ bayesovskÃ¡ sÃ­Å¥ modelu ÄasovÄ› vyvÃ­jejÃ­cÃ­ jev, tedy pravdÄ›podobnostnÃ­ model v Äase
    - kaÅ¾dÃ½ ÄasovÃ½ krok mÃ¡ kopii stejnÃ½ch promÄ›nnÃ½ch a vtahÅ¯ a mezi jednotlivÃ½mi ÄasovÃ½mi kroky jsou pÅ™echodovÃ© hrany, kterÃ© modelujÃ­ vÃ½voj v Äase
        - v tom popisu je zase $P(X_0)$, pÅ™echodovÃ½ model a senzorovÃ½ model
    - kaÅ¾dÃ¡ promÄ›nnÃ¡ zÃ¡visÃ­ jen na minulÃ©m stavu, ne na celÃ© historii a pozorovÃ¡nÃ­ zÃ¡visÃ­ jen na aktuÃ¡lnÃ­m skrytÃ©m stavu

- kdyÅ¾ je chceme porovnat, tak HMM (kde mÃ¡me jenom jednu skrytou promÄ›nnou a jednu senzorovou promÄ›nnou v kaÅ¾dÃ©m Äase, jejÃ­Å¾ chovÃ¡nÃ­ ovlivÅˆuje ta jedna promÄ›nnÃ¡) je speÅ¡l pÅ™Ã­pad DBN (umoÅ¾Åˆuje vÃ­ce stavovÃ½ch promÄ›nnÃ½ch v jednom Äase a kaÅ¾dÃ¡ mÃ­t vlastnÃ­ rodiÄe) a DBN mÅ¯Å¾e bÃ½t encoded jako HMM 
    - kdyÅ¾ by mÄ›l DBN 20 boolean promÄ›nnÃ½ch, tak v HMM bude jedna velikosti $2^{20}$ moÅ¾nÃ½ch hodnot a pÅ™echodovÃ½ model musÃ­ uchovÃ¡vat pravdÄ›podobnostnÃ­ pÅ™echod mezi kaÅ¾dou dvojicÃ­ tÄ›chto sloÅ¾enÃ½ch stavÅ¯
    - v DBN ale nemusÃ­me modelovat pÅ™echod mezi vÅ¡emi kombinacemi 
        - rozdÄ›lujeme totiÅ¾ ten obrovskÃ½ problÃ©m na malÃ© tabulky, kde ty promÄ›nnÃ© modeluju samostatnÄ› a kaÅ¾dÃ¡ mÃ¡ svÅ¯j vlastnÃ­ lokÃ¡lnÃ­ pÅ™echodovÃ½ model

### decision making
#### formalize rationality via maximum expected utility principle
- racionÃ¡lnÃ­ agent je takovÃ½, kterÃ½ se rozhoduje tak, aby maximalizoval svou oÄekÃ¡vÃ¡nou uÅ¾iteÄnost - zvolÃ­ tu akci, kterÃ¡ mÃ¡ nejvÄ›tÅ¡Ã­ oÄekÃ¡vanÃ½ pÅ™Ã­nost
- pro kaÅ¾dÃ½ vÃ½sledek $s$ spoÄÃ­tÃ¡me s jakou pravdÄ›podobnostÃ­ mÅ¯Å¾e nastat vzhledem k akci a dÅ¯kazÅ¯m $P(s|a,e)$ a potÃ© to vynÃ¡sobÃ­me uÅ¾iteÄnostÃ­ vÃ½sledku 
    - po seÄtenÃ­ toho vÅ¡eho odstaneme prÅ¯mÄ›rnou oÄekÃ¡vÃ¡nou hodnotu, kterou by nÃ¡m akce pÅ™inesla
    $EU(aâˆ£e)=âˆ‘P(Result(a)=sâˆ£a,e)â‹…U(s)$
- agent by mÄ›l vybrat tu akci, kterÃ¡ nÃ¡m pÅ™inese tu nejvÄ›tÅ¡Ã­ oÄekÃ¡vÃ¡nou uÅ¾iteÄnost
- Äasto se ale stane, Å¾e neznÃ¡me pÅ™esnÃ© ÄÃ­slo uÅ¾itku a mÅ¯Å¾eme porovnÃ¡vat dvÄ› akce dohromady; prostÄ› je lepÅ¡Ã­ je srovnat, neÅ¾ jim dÃ¡vat pÅ™esnÃ© ÄÃ­slo
    - nejhorÅ¡Ã­mu vÃ½sledku nastavÃ­m uÅ¾iteÄnost na nulu a nejlepÅ¡Ã­mu na jedniÄku
    - potom se agenta ptÃ¡m, jestli chce nÄ›jakou moÅ¾nost S anebo si nÃ¡hodnÄ› vybrat z dvojice (max_moÅ¾nost a min_moÅ¾nost) a mÄ›nÃ­m pravdÄ›podobnost, s jakou si nÃ¡hodnÄ› zvolÃ­m tu lepÅ¡Ã­ moÅ¾nost, dokud agent neÅ™ekne 'mah, je mi to jedno, jestli budu mÃ­t S nebo nÄ›co z tÃ© dvojice'
        - jakmile to Å™ekne, tak ta pravdÄ›podobnost $p$ pro maximÃ¡lnÃ­ moÅ¾nost je hodnota uÅ¾iteÄnosti pro ten vÃ½sledek S $U(S)=p$

#### define decision network
- je to rozÅ¡Ã­Å™enÃ­ bayesovskÃ½ch sÃ­tÃ­, kterÃ© kromÄ› nÃ¡hodnÃ½ch promÄ›nnÃ½ch obsahujÃ­ takÃ© rozhodovgacÃ­ uzly (volby, kterÃ© agent mÅ¯Å¾e dÄ›lat) a uÅ¾itkovÃ© uzly (kvantifikace, jak moc danÃ½ vÃ½sledek agent preferuje)
![alt text](image-2.png)
- pÅ™i racionÃ¡lnÃ­m rozhodovÃ¡nÃ­ vyuÅ¾Ã­vÃ¡me Maximum expected utility (MEU), tedy vybereme akci, kterÃ¡ maximalizuje oÄekÃ¡vÃ¡nou uÅ¾iteÄnost
    - nastavÃ­m aktuÃ¡lnÃ­ stav (kdyÅ¾ mÃ¡me nÄ›jakÃ¡ pozorovÃ¡nÃ­)
    - a pro kaÅ¾dou moÅ¾nou hodnotu rozhodovacÃ­ho uzlu spoÄÃ­tÃ¡m posteriÃ³znÃ­ pravdÄ›podobnost vÅ¡ech promÄ›nnÃ½ch, co ovlivÅˆujÃ­ uÅ¾iteÄnost a nakonec spoÄÃ­tÃ¡m tu oÄekÃ¡vÃ¡nou uÅ¾iteÄnost
    - vyberu tu akci s nejvyÅ¡Å¡Ã­ uÅ¾iteÄnostÃ­

#### define a sequential decision problem
- mÃ¡me plnÄ› pozorovatelnÃ© nedeterministickÃ© prostÅ™edÃ­ a hledÃ¡me plÃ¡n, jak zÃ­skat co nejvÄ›tÅ¡Ã­ uÅ¾itek

- markov desicion process (MDP) je matematickÃ½ rÃ¡men pro prostÅ™ednÃ­, kterÃ© je plnÄ› pozorovatelnÃ©, stochastickÃ© a markovskovskÃ© (budoucnost zÃ¡visÃ­ pouze na pÅ™Ã­tomnosti, ne na minulosti)
    - obsahuje stavy - vÅ¡echny moÅ¾nÃ© situace, do kterÃ½ch se agent mÅ¯Å¾e dostat
    - akce, kterÃ© agent mÅ¯Å¾e dÄ›lat
    - pÅ™echodovÃ½ model $P(s'|s,a)$, tedy pravdÄ›podobnost, Å¾e se akce $a$ ve stavu $s$ vede do stavu $s'$
    - odmÄ›novÃ¡ funkce $R(s)$, coÅ¾ je krÃ¡tkodobÃ¡ odmÄ›na, kterou agent dostane, kdyÅ¾ se dostane do stavu $s$
    - $Î³$, coÅ¾ je hodnota, kterÃ¡ nÃ¡m Å™Ã­kÃ¡, jak moc si cenÃ­me budoucÃ­ch odmÄ›n (pokud je to blÃ­Å¾ nule, tak jedeme na blÃ­zkÃ© odmÄ›ny)
        - pouÅ¾ito v rovnici $U[(s_0,s_1,...)]=R(s_0) + Î³*R(s_1)*Î³^2 +R(s_2) +...$, kde $U()$ je dlouhodobÃ¡ odmÄ›ÅˆovacÃ­ funkce

- cÃ­l agenta je maximalizovat oÄekÃ¡vÃ¡nou uÅ¾iteÄnost (uÅ¾iteÄnost je koneÄnÃ¡, protoÅ¾e ta Å™ada nahoÅ™e konverguje k nule)
    - politika $\pi(s)$ mi Å™Ã­kÃ¡, jakou akci mÃ¡m provÃ©st ve stavu $s$ a my hledÃ¡me tu optimÃ¡lnÃ­ politiku $\pi*(s)$, kterÃ¡ nÃ¡m maximalizuje dlouhodobou odmÄ›nu
    - spoÄÃ­st danou politiku $\pi(s)$ se dÃ¡ jako  $U_Ï€(s)=E[âˆ‘Î³^iâ‹…R(S_i)]$
    - a my z nich potÃ© vybÃ­rÃ¡me tu nejlepÅ¡Ã­ pro zaÄÃ¡tek $\pi^*(s)=maxU_\pi(s)$
        - ale pokud je prostÅ™edÃ­ markovskÃ©, tak mi nejde o poÄÃ¡teÄnÃ­ stav, existuje jedna univerzÃ¡lnÃ­ optimÃ¡lnÃ­ politika
    - optimÃ¡lnÃ­ politika se podÃ­vÃ¡ na stav $s$ a zjistÃ­, jakÃ¡ akce bude mÃ­t nejvÄ›tÅ¡Ã­ uÅ¾itek $Ï€^âˆ—(s)=max_aâˆ‘P(sâ€²âˆ£s,a)â‹…U(sâ€²)$ 
        - projdu prostÄ› vÅ¡echny moÅ¾nosti, co mohou nastat, a vyberu tu moÅ¾nost, kterÃ¡ mi dÃ¡ nejvÄ›tÅ¡Ã­ vÃ½sledek

- tak jsme se dobrali k bellmanovÄ› rovnici, kterÃ¡ mi Å™Ã­kÃ¡ okamÅ¾itou odmÄ›nu a jakou odmÄ›nu dostanu v budoucnosti $U(s)=R(s)+Î³â‹…maxâ€²âˆ‘P(sâ€²âˆ£s,a)â‹…U(sâ€²)$, kde $U(s)$ je dlouhodobÃ¡ uÅ¾iteÄnost, $R(s)$ je okamÅ¾itÃ¡ a ten zbytek je nejlepÅ¡Ã­ budoucÃ­ stav
- value iteration je algoritmus, kdy za $U(s)$ dosadÃ­m nÄ›jakou hodnotu a opakuju bellmanovu rovnici, dokud se mi ta hodnota neustÃ¡lÃ­ okolo nÄ›jakÃ© hodnoty
    - ta hodnota je moje optimÃ¡lnÃ­ politika
    - $U_{i+1}(s)â† R(s)+Î³â‹…maxâ€²âˆ‘P(sâ€²âˆ£s,a)â‹…U_i(sâ€²)$

- policy iteration hledÃ¡ optimÃ¡lnÃ­ politiku, aby byl dlouhodobÃ½ zisk co nejvÄ›tÅ¡Ã­
    - nejdÅ™Ã­ve si spoÄÃ­tÃ¡m, jak dobrÃ¡ je aktuÃ¡lnÃ­ politika
    - a pak se podÃ­vÃ¡m, jestli neexistuje lepÅ¡Ã­ akce
    - tohle provÃ¡dÃ­m, dokud nenajdu tu nejlepÅ¡Ã­ (uÅ¾ se to nemÄ›nÃ­)

#### formulate Partially Observable MDP and show how to solve it.
- agent zde pÅ™esnÄ› nevidÃ­ stav, nemÅ¯Å¾eme pÅ™Ã­mo vyuÅ¾Ã­t $\pi(s)$, protoÅ¾e nevÃ­, co $s$ je
- poÅ™Ã¡d budeme mÃ­t transition model, akce, odmÄ›ny a sensor model, ale nevÃ­m, co pÅ™esnÄ› je nÃ¡Å¡ nynÄ›jÅ¡Ã­ stav, a na to pouÅ¾ijeme odhady (belief states)
    - vyÅ™eÅ¡Ã­me MDP pÅ™es belief states

### game theory
#### explain core properties of environment and information needed to apply adversarial search
- adversarial search se pouÅ¾Ã­ve v prostÅ™edÃ­ch, kde existuje protivnÃ­k, kterÃ½ pÅ¯sobÃ­ proti nÃ¡m
    - prostÄ› hry pro dva
    - snaÅ¾Ã­ se najÃ­t nejlepÅ¡Ã­ tah pro jednoho hrÃ¡Äe
- abychom tohle mohli pouÅ¾Ã­t, musÃ­ to prostÅ™edÃ­ splÅˆovat nÄ›jakÃ© vlastnosti
    - kaÅ¾dÃ¡ akce mÃ¡ pÅ™edvÃ­datelnÃ½ vÃ½sledek
    - je to plnÄ› pozorovatelnÃ© - oba hrÃ¡Äi vidÃ­ celou hernÃ­ situaci
    - diskrÃ©tnÃ­ stavy - hernÃ­ pozice je jednoznaÄnÄ› definovanÃ¡
    - dva hrÃ¡Äi
    - to, co chce jeden, je pro druhÃ©ho ta nejhorÅ¡Ã­ moÅ¾nost
#### minimax a alfa beta pruning
- minimax je strom hernÃ­ch stavÅ¯, kdy v jednom chci minimalizovat (tah spoluhrÃ¡Äe) a v druhÃ©m maximalizovat (mÅ¯j tah)
- je to ale hroznÄ› nepraktickÃ©, stromy jsou velkÃ© (rostou exponenciÃ¡lnÄ›), a tak to chceme oÅ™ezat
- alfa-beta pruning odÅ™ezÃ¡vÃ¡ ty vÄ›tve, u kterÃ½ch mi je jasnÃ©, Å¾e tam nebude lepÅ¡Ã­ vÃ½sledek
    - alfa je nejlepÅ¡Ã­ hodnota, kterou mÅ¯Å¾e MAX garantovat
    - beta je nejlepÅ¡Ã­ hodnota, kterou mÅ¯Å¾e MIN garantovat
    - pokud $\beta â‰¤ \alpha$, tak tu vÄ›tev zaÅ™Ã­zneme
![alt text](image-3.png)

    - hodÃ­ se to, kdyÅ¾ nechceme projet celÃ½ strom - za stejnÃ½ Äas se dostaneme do dvojnÃ¡sobnÃ© hloubky

- kdyÅ¾ nemÅ¯Å¾eme prohledat celÃ½ strom, tak pouÅ¾ijeme alfa-beta pruning, ale to samo o sobÄ› nestaÄÃ­ - pÅ™idÃ¡me hodnotÃ­cÃ­ funkci
    - v nÄ›jakÃ© hloubce ten strom zaÅ™Ã­znu a aproximuju uÅ¾iteÄnost pomocÃ­ evaluation function (heuristika)
        - my chceme co nejvÄ›tÅ¡Ã­ hodnotu, nepÅ™Ã­tel co nejmenÅ¡Ã­
        - pÅ™Ã­klad: Å¡achy - #peÅ¡Ã¡kÅ¯, celkovÃ½ poÄet, je Å¾ivÃ¡ krÃ¡lovna??
            - kaÅ¾dÃ© tÃ© moÅ¾nosti pÅ™iÅ™adÃ­m vÃ¡hu a spoÄÃ­tÃ¡m celkovou vÃ¡hu jejich souÄtem

#### stochastickÃ© hry
- stochastickÃ© hry jsou takovÃ©, kde je i prvek nÃ¡hody a minimax s tÃ­m musÃ­ pracovat (Å™Ã­kÃ¡ se tomu expected minimax)
    - vyuÅ¾Ã­vÃ¡ pravdÄ›pdobnost, kdy do stromu pÅ™idÃ¡me chance uzly
    - pokud jsme v terminÃ¡lu node, tak vrÃ¡tÃ­me vÃ½sledek (utility)
    - pokud jsme v max, tak pro vÅ¡echny dÄ›ti spoÄÃ­tÃ¡me hodnotu a vybereme nejlepÅ¡Ã­ 
    - pokud jsme v min, tak vezmeme tu nejmenÅ¡Ã­
    - pokud jsme v chance node, tak seÄteme vÃ¡Å¾enÃ© prÅ¯mÄ›ry vÅ¡ech nÃ¡stupcÅ¯ a zÃ­skÃ¡me $\sum P(tah)*uÅ¾iteÄnost$ pÅ™es vÅ¡echny tahy
        - do chance node se jedou tahy a z nich se udÄ›lÃ¡ vÃ¡Å¾enÃ½ prÅ¯mÄ›r a tahle hodnota se potÃ© vyuÅ¾Ã­vÃ¡ v urÄovÃ¡nÃ­ minimaxu
        ![alt text](image-4.png)

#### single move hry
- pri single-moves games si kaÅ¾dÃ½ hrÃ¡Ä vybere jednu strategii a najednou se vÅ¡ichni odhalÃ­ (ty svoje strategie)
    - payoff funkce udÄ›luje hrÃ¡Äi utility podle toho, jak hrÃ¡li ostatnÃ­ hrÃ¡Äi(pro kaÅ¾dou kombinaci mÅ¯Å¾eme vytvoÅ™it vÃ½platnÃ­ matici)
    - strategie mÅ¯Å¾e bÃ½t deterministickÃ¡ (pure; hrÃ¡Ä vybere jednu akci) anebo pravdÄ›podobnostnÃ­ (mixed; hrÃ¡Ä pÅ™iÅ™adÃ­ pravdÄ›podobnost ke vÅ¡em tahÅ¯m)
    - chceme pÅ™iÅ™adit kaÅ¾dÃ©mu hrÃ¡Äi racionÃ¡lnÃ­ strategii
- dominantnÃ­ strategie - nejlepÅ¡Ã­ volba bez ohledu na ostatnÃ­ hrÃ¡Äe
- nash equilibrium je ve chvÃ­li, kdy si nikdo nemÅ¯Å¾e zlepÅ¡it svou vÃ½sledek zmÄ›nou strategie 
- pareto dominance je takovÃ¡ strategie, pokud pro vÅ¡echny hrÃ¡Äe je stejnÄ› dobrÃ¡ a pro nÄ›koho lepÅ¡Ã­ neÅ¾ jinÃ¡ strategie
- hrÃ¡Äovo dilema 
![](image-5.png)
    - racionÃ¡lnÃ­ strategie je testify, ale pareto optimal je refuse refuse (myslÃ­m)

- maximin strategie je to, kdyÅ¾ chceme bÃ½t opatrnÃ­, a tak si pro kaÅ¾dou strategii najdu jejÃ­ nejhorÅ¡Ã­ moÅ¾nost a z tÄ›ch strategiÃ­ si vyberu, jejÃ­Å¾ nejhorÅ¡Ã­ moÅ¾nost nenÃ­ tak hroznÃ¡ 
    - pouÅ¾Ã­vÃ¡ se, kdyÅ¾ o protivnÃ­kovi nic nevÃ­m
    - pÅ™Ã­klad na two finger morra, kde evÅ¾en ukazuje svÅ¯j tah jako prvnÃ­ a vybÃ­rÃ¡ strategii, aby minimalizoval ztrÃ¡tu
        - vybere si, Å¾e zahraje 1 (s pravdÄ›podobnostÃ­ $p$) a hrÃ¡Ä reaguje, aby mu co nejvÃ­ce uÅ¡kodil 
![alt text](image-6.png)
            - pokud zahraje 1, tak $U_1(p)=pâ‹…(âˆ’2)+(1âˆ’p)â‹…3=âˆ’2p+3âˆ’3p=3âˆ’5p$
            - pokud zahraje 2, tak $U_2(p)=pâ‹…1+(1âˆ’p)â‹…(âˆ’4)=pâˆ’4+4p=5pâˆ’4$
        - chceme minimalizovat maximum tÄ›chto hodnot, takÅ¾e to dÃ¡me do rovnosti a dostaneme $p=7/12$
            - hledali jsme takovou pravdÄ›podobnost pro EvÅ¾ena, abychom zajistili, Å¾e jeho vÃ½plata bude co nejlepÅ¡Ã­ v tom nejhorÅ¡Ã­m pÅ™Ã­padÄ› (Olga zvolÃ­ pro EvÅ¾ena tu nejhorÅ¡Ã­ moÅ¾nost)
            - po dosazenÃ­ dostÃ¡vÃ¡me, Å¾e to bude $-1/12$, protoÅ¾e to je z pohledu Olgy, pro kterou je to $1/12$

- repeated games je to ve chvÃ­li, kdy se ta kola opakujÃ­ 
    - 100x vÄ›zÅˆovo dilema, tak stejnÄ› bude nejracionÃ¡lnÄ›jÅ¡Ã­ vÅ¾dy prÃ¡sknout
    - pokud bude ale dalÅ¡Ã­ kolo na 99 %, tak to bude jinÃ©
        - perpetual punishment - odmÃ­tÃ¡m se pÅ™iznat tak dlouho, dokud neprÃ¡skne ten druhÃ½, pak praskÃ¡m
        - tit-for-tat - zaÄnu s odmÃ­tÃ¡m a potÃ© opakuju tah hrÃ¡Äe z minulÃ©ho kole (tohle je velmi efektivnÃ­ proti spoustÄ› strategiÃ­m)

#### mechanism design: explain classical auctions and tragedy of commons
- mechanism design nÃ¡m Å™Ã­kÃ¡, jak navrhovat pravidla, abychom motivovali hrÃ¡Äe k Å¾Ã¡doucÃ­mu chovÃ¡nÃ­ (vlÃ¡da chce snÃ­Å¾it zneÄiÅ¡tÄ›nÃ­, a tak pÅ™ijde s emisnÃ­mi povolenkami)
- anglickÃ¡ aukce - jdeme odzdola nahoru
    - mÃ¡ to dominantnÃ­ strategii (pÅ™ihazovat, dokud to nenÃ­ nad moji cenu), ale kdyÅ¾ je tam nÄ›kdo bohatÃ½, odradÃ­ to ostatnÃ­ a taky je to nÃ¡roÄnÃ© na spojenÃ­
- dutch aukce - jdeme odshora dolÅ¯
    - rychlÃ©
- sealed-bid auction - ÄlovÄ›k udÄ›lÃ¡ nabÃ­dku nezÃ¡visle na ostatnÃ­ch a ta vyhrÃ¡vÃ¡
    - nenÃ­ tu Å¾Ã¡dnÃ¡ simple dominantnÃ­ strategie (pokud vÄ›Å™Ã­m, Å¾e max nabÃ­dka je $b$, kde $b$ je niÅ¾Å¡Ã­ neÅ¾, co jÃ¡ chci max utratit, tak dÃ¡m $b + \varepsilon$)
- vickrey aukce - dÃ¡t nabÃ­dku nezÃ¡visle na ostatnÃ­c, vyhrÃ¡vÃ¡ nejvyÅ¡Å¡Ã­ hodnota, ale platÃ­ druhou nejvyÅ¡Å¡Ã­ hodnotu
    - primÃ¡rnÃ­ strategie je prostÄ› vsadit tu svoji hodnotu

- the tragedy of commons - vÃ­c lidÃ­ sdÃ­lÃ­ omezenÃ½ zdroj, ale nikdo za jeho vyuÅ¾Ã­vÃ¡nÃ­ nemusÃ­ platit, rychle se vyuÅ¾ije celÃ½, a to vede k menÅ¡Ã­mu obnosu vÅ¡ech ÃºÄastnÃ­kÅ¯
    - Å™eÅ¡enÃ­: regulace, privatizace, zpoplatnÄ›nÃ­ 
    - vickrey-clarks-groves algoritmus: lidÃ© nahlÃ¡sÃ­, kolik jsou za tu sluÅ¾bu ochotni platit, vybere se ta skupina lidÃ­, kterÃ¡ by platila nejvÃ­ce
        - ti budou platit tu nejvyÅ¡Å¡Ã­ cenu, kterÃ¡ se nedostala do podmnoÅ¾iny
        - dominantÃ­ strategie? hodit tam pravdu

### machine learning
#### define and compare types of learning
